---
title: A Simple Kalman Filter
author: admin
date: '2021-01-06'
summary: 'This purposefully simple document explains the main intuition behind Kalman filtering of measurement noise.'
slug: []
categories:
  - macroeconometrics
tags:
  - macroeconometrics
meta_img: images/image.png
description: Description for the page
bibliography: [bibliography_simpleKF.bib]
---



<p>This document is generated with {{&lt; icon name=“r-project” pack=“fab” &gt;}} -Markdown. All related files and code can be found in <a href="https://github.com/bbcon/ASimpleKalmanFilterWithR">GitHub</a></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This document provides an intuitive and simple introduction to Kalman Filtering.</p>
<p>The document is organised as follows: the first section motivates the usage of a Kalman Filter in an economic context and reviews the related theory. In particular, it focuses on the key results and ideas behind it–as we shall see, the Kalman Filter’s key idea is in reality surprisingly easy to understand. The second section applies the Kalman Filter algorithm to an extremely simple (yet not trivial) state-space model characterised by an AR(1) process as the state equation. The idea behind this is to simplify as much as possible in order to focus on the key elements that will then reveal useful to understand more complex problems.</p>
<p>This document is mostly based on lecture Notes from Prof. Mark Watson (Princeton University) given as part of the program for Beginning Doctoral students in Economics in Gerzensee taught in March 2020 ((<span class="citation">Watson (2020)</span>). The lecture notes are themselves heavily based on <span class="citation">Hamilton (1995)</span>.</p>
</div>
<div id="the-kalman-filter-for-economics-motivation-theory-and-intuition" class="section level1">
<h1>The Kalman Filter for Economics : Motivation, theory, and intuition</h1>
<div id="motivation-and-simple-intuition" class="section level2">
<h2>Motivation and simple intuition</h2>
<p>We typically observe economic data over time. These data, however, are likely to be noisy. The Kalman Filter (KF) (in an economic context) usually considers two types of potential noise: measurement and structural noise. More precisely, the KF makes the assumption that observed data are subject to measurement errors and are in reality generated by an unobservable process. This unobservable process is known as the state (or transition) equation and is subject to some noise that we can refer to as “structural”. In that context, the KF is an algorithm that allows to filter out the measurement noise in order to learn more about the true generating process of the data. In particular, it can be used to learn more about so-called structural shocks which are of crucial importance in DSGE models for instance.</p>
<p>The Kalman Filter was first used in physics but is now widely popular in economics.</p>
</div>
<div id="the-theory" class="section level2">
<h2>The theory</h2>
<p><strong>Notation</strong></p>
<p>The general form of the Kalman filter as presented in Hamilton Chapter 13 (add ref) is given by a “measurement equation”:
<span class="math display">\[\begin{align*}
y_t = A&#39;x_t + H&#39;\xi_t + w_t 
\end{align*}\]</span>
With <span class="math inline">\(E(w_tw_t&#39;)=Q\)</span></p>
<p>And a transition (or state) equation:
<span class="math display">\[\begin{align*}
\xi_t = F\xi_{t-1} + v_t 
\end{align*}\]</span>
With <span class="math inline">\(E(v_tv_t&#39;)=R\)</span>.</p>
<p>In words:</p>
<ul>
<li><span class="math inline">\(y_t\)</span> is the vector of observed variables (i.e. the data)</li>
<li><span class="math inline">\(x_t\)</span> is a vector of deterministic components (we won’t spend time on it in this document)</li>
<li><span class="math inline">\(\xi_t\)</span> is the unobserved “state” variables</li>
<li><span class="math inline">\(w_t\)</span> and <span class="math inline">\(v_t\)</span> are unobserved, mutually and serially uncorrelated noise variables (which usually follow a Normal Distribution)</li>
<li><span class="math inline">\(A, H, R, F,\)</span> and <span class="math inline">\(Q\)</span> are non-random “system” variables matrices that may depend on unknown parameters (some of them can be retrieved using Maximum Likelihood estimations)</li>
</ul>
<p>The general system defined by these two equations is flexible and can accomodate a variety of representation. For instance, a standard AR(p) process fits into the general notation in the following way:</p>
<p>Let <span class="math inline">\(y_t \sim AR(p)\)</span>, that is:
<span class="math display">\[\begin{align*}
y_t = \phi_1 y_{t-1} + \phi_2y_{t-2} + ... + \phi_py_{t-p} + \epsilon_t
\end{align*}\]</span>
This process can be represented as a “state-space” model in the following way:
<span class="math display">\[\begin{align*}
&amp;\xi_t = \begin{bmatrix}
y_t \\ y_{t-1} \\ \vdots \\ y_{t-p+1}
\end{bmatrix} \\
&amp;F= \begin{bmatrix} 
\phi_1 &amp; \phi_2 &amp; \ldots &amp; \phi_{p-1} &amp; \phi_p \\
1 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 \\
0 &amp; 1 &amp;&amp;&amp; 0 \\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
0 &amp;&amp;&amp;1&amp;0
\end{bmatrix} \\
&amp;v_t = \begin{bmatrix}
\epsilon_t \\ 0 \\ \vdots \\ 0
\end{bmatrix}
\end{align*}\]</span></p>
<p>And <span class="math inline">\(w_t=0, A=0,\)</span> and <span class="math inline">\(H&#39;=\begin{bmatrix}1 &amp; 0 &amp; \ldots &amp; 0\end{bmatrix}\)</span></p>
<div id="procedure-and-idea-of-the-kalman-filter" class="section level3">
<h3>Procedure and idea of the Kalman Filter</h3>
<p>Notation:</p>
<ul>
<li><span class="math inline">\(y_{1:t} = \left\{y_i\right\}^t_{i=1}\)</span></li>
<li><span class="math inline">\(\xi_{t|k} = E(\xi_t|y_{1:k})\)</span></li>
<li><span class="math inline">\(P_{t|k}=Var(\xi_{t}|y_{1:k})\)</span></li>
</ul>
<p>In words, the Kalman filter is a recursive algorithm that constructs <span class="math inline">\(\xi_{t|t}\)</span> and <span class="math inline">\(P_{t|t}\)</span> from known values in <span class="math inline">\(t\)</span>, that is <span class="math inline">\(y_t,x_t, \xi_{t-1|t-1}, P_{t-1|t-1}\)</span>.</p>
<p>To derive the filter, we assume that both <span class="math inline">\(w_t\)</span> and <span class="math inline">\(v_t\)</span> follow iid Gaussian process, that is:</p>
<p><span class="math display">\[\begin{bmatrix}
w_t \\ v_t
\end{bmatrix} 
 \sim N \left(\begin{bmatrix}
0 \\
0\end{bmatrix}, \begin{bmatrix}
R &amp; 0\\
0 &amp; Q \end{bmatrix}\right)\]</span></p>
<p>This notably implies that both <span class="math inline">\(y_t\)</span> and <span class="math inline">\(\xi_t\)</span> follow a <em>joint</em> Normal distribution. Since errors are Gaussian, the best estimator (in the sense that it minimises the mean squared error) is given by the conditional expectation.</p>
<p>To find the conditional expectation of <span class="math inline">\(\xi_t\)</span> and <span class="math inline">\(y_t\)</span> (that is <span class="math inline">\(\xi_{t|t}\)</span> and <span class="math inline">\(y_{t|t}\)</span>), we can use the following theorem of the conditional distribution of a multivariate normal:</p>
<p>Suppose that:</p>
<p><span class="math display">\[\begin{bmatrix}
z_1 \\ z_2
\end{bmatrix} \sim N \left(\begin{bmatrix}\mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix}\Sigma_{11} &amp;\Sigma_{12}\\\Sigma_{21}&amp;\Sigma_{22}\end{bmatrix}\right)\]</span></p>
<p>Then:</p>
<p><span class="math display">\[E(z_1|z_2) = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(z_2 - \mu_2) \\
Var(z_1|z_2) = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\]</span></p>
<p>The application of this theorem is the <strong>key idea</strong> of the Kalman Filter.</p>
<p>Defining <span class="math inline">\(z_1 = \xi_t\)</span> and <span class="math inline">\(z_2 = y_t\)</span>, and recognizing that <span class="math inline">\(\xi_t\)</span> and <span class="math inline">\(y_t\)</span> are jointly Normal conditional on past values, we can write the following:</p>
<p><span class="math display">\[\begin{bmatrix}
\xi_t \\ y_t
\end{bmatrix} \Bigg | y_{1:t-1}\sim \mathcal{N}\left(\begin{bmatrix}
\xi_{t|t-1} \\ y_{t|t-1}
\end{bmatrix}, \begin{bmatrix}
P_{t|t-1} &amp; \Sigma_{\xi,y|t-1} \\\Sigma_{\xi,y|t-1} &amp; \Sigma_{yy|t-1}
\end{bmatrix}\right)\]</span></p>
<p>Using the formula of the conditional normal:</p>
<p><span class="math display">\[\begin{align*}
\xi_{t|t}= \xi_{t|t-1} + \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}(y_t-y_{t|t-1}) \\
P_{t|t} = P_{t|t-1} - \Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1}\Sigma_{\xi,y|t-1}
\end{align*}\]</span></p>
<p>Assuming <span class="math inline">\(\xi_{t-1|t-1}\)</span> and <span class="math inline">\(P_{t-1|t-1}\)</span> are known, the Kalman Filter algorithm is given by:</p>
<p><span class="math display">\[\begin{align*}
&amp;\xi_{t|t-1} = F\xi_{t-1|t-1} \\
&amp;y_{t|t-1} = A&#39;x_t + H&#39;\xi_{t|t-1} \\
&amp;P_{t|t-1} = FP_{t-1|t-1}F&#39; + Q \\
&amp;\Sigma_{yy|t-1} = H&#39;P_{t|t-1}H + R \equiv h_t \\
&amp;\Sigma_{\xi,y|t-1}\Sigma_{yy|t-1}^{-1} = P_{t|t-1}H h_t^{-1} \equiv K_t \\
&amp;\eta_t = y_t-y_{t|t-1}
\end{align*}\]</span></p>
<p>Applying the theorem, we get:</p>
<p><span class="math display">\[\begin{align*}
&amp;\xi_{t|t} = \xi_{t|t-1} + K_t \eta_t\\
&amp;P_{t|t} = P_{t|t-1} - K_t H&#39;P_{t|t-1}
\end{align*}\]</span></p>
<p>If the process is covariance stationary, we can initialize it by assuming <span class="math inline">\(\xi_{0|0}=0\)</span> and using a reasonable estimate of <span class="math inline">\(P_{0|0}\)</span>. We can then retrieve <span class="math inline">\(\xi_{t|t}\)</span>, and <span class="math inline">\(P_{t|t}\)</span> for all <span class="math inline">\(t&gt;0\)</span> applying this procedure recursively.</p>
</div>
</div>
<div id="application" class="section level2">
<h2>Application</h2>
<p>To better understand the algorithm let us consider the following (uni-dimensional) simple example.</p>
<p>The state space model is of the form:</p>
<p><span class="math display">\[\begin{align*}
&amp;y_t = \phi \xi_t + w_t \\
&amp;\xi_t = \xi_{t-1} + v_t
\end{align*}\]</span>
For simplicity, we assume that the transition equation is known with certainty and that <span class="math inline">\(\phi=\)</span> 0.8. Moreover, we assume that <span class="math inline">\(w_t\)</span> and <span class="math inline">\(v_t\)</span> are i.i.d and independent of one another with <span class="math inline">\(\sigma^2_w=\)</span> 1 and <span class="math inline">\(\sigma^2_v=\)</span> 1.</p>
<p>With that information, we can generate <span class="math inline">\(y_t\)</span>. Next graph provides a graphical representation.</p>
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The last step is to initalize the loop. We assume <span class="math inline">\(\xi_{0|0}=0\)</span> because the process is covariance stationary, and <span class="math inline">\(P_{0|0}=1\)</span> arbitrarily.</p>
</div>
<div id="the-algorithm" class="section level2">
<h2>The algorithm</h2>
<p>Since <span class="math inline">\(\xi_{0|0}\)</span> and <span class="math inline">\(P_{0|0}\)</span> are known, we can recursively compute <span class="math inline">\(\xi_{t|t}\)</span> and <span class="math inline">\(P_{t|t}\)</span> for <span class="math inline">\(t&gt;0\)</span> using the Kalman Filter algorithm:</p>
<p><span class="math display">\[\begin{align*}
&amp;\xi_{t|t-1}= \phi \xi_{t-1|t-1} \\
&amp;y_{t|t-1} = \xi_{t|t-1} \\
&amp;P_{t|t-1} = \phi^2P_{t-1|t-1} + \sigma_v^2 \\
&amp;h_t \equiv Var(y_t|t-1)  = P_{t|t-1} + \sigma^2_w \\
&amp;K_t = Cov(\xi_t,Y_t|t-1)\times h_t = P_{t|t-1}\times h_t^{-1} \\
&amp;\eta_t = y_t-y_{t|t-1}
\end{align*}\]</span></p>
<p>Using this, we can get our next period KF forecast:</p>
<p><span class="math display">\[\begin{align*}
\xi_{t|t} = \xi_{t|t-1} + K_t\times \eta_t \\
P_{t|t} = P_{t|t-1} - K_t\cdot Cov(\xi_t,Y_t|t-1)
\end{align*}\]</span></p>
<p>See R-script for its implementation in R.</p>
<p>The next graph provides a graphical representation resulting from this procedure. The blue line represents <span class="math inline">\(\xi_{t|t}\)</span> for all <span class="math inline">\(t\)</span> while the black one is the generated data.</p>
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>To check the validity of the Kalman Filter, one can for instance plot the true process (thus without measurement errors) against our freshly derived Kalman Filter. Note that, in general, this cannot be done as one typically does not the true generating process.</p>
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>To further check the validity of our Kalman Filter, we can plot the actual measurement noise (which we randomly generated) with the KF measurement noise (which is retrieved by subtracting our Kalman forecast to the the generated data). As we can see on the next plot, the two match quite closely, indicating that our Kalman Filter was able to filter out the measurement noise quite succesfully.</p>
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Another check is to compare the actual state noise (or structural shocks depending on the context) with the ones implied by the Kalman Filter. In general, this is not something that can be done as the process that generates the data is not known with certainty. Here, as we generated the data ourselves, it is however possible to perform such a test. State noise implied by the Kalman Filter is equal to our estimate of <span class="math inline">\(\xi_{t|t}\)</span> minus the actual AR(1) process without the state (or structural) noise. Next plot provides a graphical representation.</p>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>This document provides a purposefully very simple application of a Kalman Filter. My wish was to be able to focus on the key ideas behind it. More complex version of this problem would for example be to consider a multi-dimensional state-space model with unknown parameters that would require to be estimated using a Maximum Likelihood estimation. I would, however, argue that considering such settings would not have added much value given that the aim of this document was to understand what is a Kalman Filter and what it does in words. I hope it is clearer now.</p>
<p>If you notice any typos, errors, or omissions, feel free to send a mail to this address: <a href="mailto:brendan.berthold@gmail.com" class="email">brendan.berthold@gmail.com</a>.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-hamilton_1994">
<p>Hamilton, James D. 1995. “Time Series Analysis.” <em>International Journal of Forecasting</em> 11 (3): 494–95. <a href="https://ideas.repec.org/a/eee/intfor/v11y1995i3p494-495.html">https://ideas.repec.org/a/eee/intfor/v11y1995i3p494-495.html</a>.</p>
</div>
<div id="ref-watson_2020">
<p>Watson, Mark. 2020. “Program for Beginning Doctoral Students in Economics.” Gerzensee.</p>
</div>
</div>
</div>
