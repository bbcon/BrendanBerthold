---
title: 'K-fold cross validation for time series in R'
author: Brendan Berthold
date: '2021-02-27'
slug: k-fold-cross-validation-for-time-series-in-r-applications-to-var-models
categories:
  - R
  - forecasting
  - Packages
tags:
  - pro
subtitle: ''
summary: 'The K-fold cross validation allows to quantify the performance of a forecasting model. This article explains the implementation of this procedure for timeseries in the context of a VAR model. It makes use of the kfoldcv4ts package that I created.'
authors: []
lastmod: '2021-02-27T13:33:42+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: 'bibliography.bib'
---

---
title: "K-fold cross validation in R : application to time series in a VAR setting"
author: "Brendan Berthold"
date: "2/27/2021"
output:
  html_document: bookdown::html_document2
  pdf_document: bookdown::pdf_document2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
```


# Introduction

```{r, warning = FALSE, include = FALSE}
library(dplyr)
Sys.setenv(`_R_S3_METHOD_REGISTRATION_NOTE_OVERWRITES_` = "false")
```


I recently wrote a little package (`kfoldcv4ts`) that assesses the fit of Vector Autoregressive (henceforth VAR) models for forecasting by performing a k-fold cross validation adapted to timeseries. I thought that it may be of interest to some people, so I decided to write a little article and share it. As of now, the function `kfoldcv4ts::accuracy.kfold` has only been tested with `varest` objects from the package `vars`. However, the function being quite simple, it should not be a problem to adapt it to other types of forecasting model.

In this article, I provide a little summary of k-fold cross validation and an application in R using a dataset consisting of several US macroeconomic variables (from @Stock2007). For illustration purpose, I evaluate the relative performance of several competing VAR models for predicting quarterly US GDP using a k-fold cross validation approach adapted to timeseries. 


I provide the commands to install my package `kfoldcv4ts` that allows to perform the k-fold cross validation for timeseries (function `accuracy.kfold`). The package makes use of several existing packages, most notably the excellent `forecast` and `vars` package.
```{r,echo=TRUE, eval = FALSE}
devtools::install_github('bbcon/kfoldcv4ts')
```



*Disclaimer: the function is meant to be easy to read such that the user can follow along each step of a k-fold cross validation. It is by no means meant to be efficient. Comments are welcome.*

To follow along the steps of this article, you need to install/load the following packages: `dplyr`, `vars`, `forecast`, and `AER`. Most of these packages are also required for installing the `kfoldcv4ts` package.

# Some theory

Judging the performance of a forecasting model is absolutely critical. A natural way to do so is to look at how well the model predictions match with the observed values. There are a variety of ways to formally quantify the performance of a forecasting model.^[There exist other measures to assess the fit of a model such as the MAE (mean absolute deviation), MPE (mean percentage error) and certainly many others. The best measure is usually not absolute and often depends on the application at hand.] Maybe the most popular is the MSE (or RMSE) which stands for (root) mean squared error. Mathematically, MSE is given by:
$$MSE = \frac{1}{n} \sum_{i}^n(y_i-\hat{f}(x_i))^2$$ 
Where $n$ is the number of observations, $y_i$ is the observed data, and $\hat{f}(x_i)$ is the estimated forecasting model expressed as a function of a number of regressors $x_i$ that help predict $y_i$. 



## Training versus test data

To evaluate the performance of a model, it is important to assess the fit on data that has not been used for estimation (out-of-sample forecasting). The reason for this is that it is very simple to improve in-sample fit by adding regressors, but this may result in overfitting and thus poor performance when the model is confronted to new data.^[For more information about this, read for example: https://en.wikipedia.org/wiki/Biasâ€“variance_tradeoff]

It is thus natural to split the data in two parts: a training and a test dataset. The former is used to estimate the model's parameters, and the latter to evaluate how well the model performs when confronted with new data.


## K-fold cross validation

K-fold cross validation is precisely a way to split the data in a training and test dataset. The easiest way to understand k-fold cross validation is certainty to look at a graphical representation:

(ref:source) [Stackexchange (Jatin Garg)](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection)

![Graphical representation of K-fold cross validation. Source: (ref:source)](img/kfoldCV.png)

In words, k-fold cross validation amounts to separate the data in k training and testing datasets. For each fold, we use the training dataset to estimate the models' parameters, and the test dataset to evaluate its out-of-sample performance. This allows to replicate a real life situation where the data used to estimation are not the ones we want to predict. Additionally, it also gives information on how new data affect the performance of the model. A MSE that does not decrease with each additional fold can be suggestive that the forecasting model is not appropriate for the data studied.


# Application to US GDP in R

We use macroeconomic time series from the `AER` package. The dataset (`USMacro`) is from the @Stock2007. It consists of 7 time series from 1957 to 2005 available at the quarterly frequency. The following commands load the data in R:
```{r, warning = FALSE, include=FALSE}
library('AER')
```

```{r}
data("USMacroSW", package = 'AER')
```


As an exercise, we decide to estimate a VAR consisting of the first 6 variables:
```{r, echo = TRUE}
t(colnames(USMacroSW)[1:6])
```


```{r,echo=TRUE}
data_VAR = USMacroSW[, c(1:6)]
```
```{r, include=FALSE}
opt.lags.AIC = vars::VARselect(data_VAR)$selection[['AIC(n)']]
```

We store the optimal number of lags according to the AIC(n) criterion (equal to `r opt.lags.AIC`) :
```{r}
opt.lags.AIC = vars::VARselect(data_VAR)$selection[['AIC(n)']]
opt.lags.HQ = vars::VARselect(data_VAR)$selection[['HQ(n)']] # for later use
```



We now use our function to perform a k-fold cross validation. The source code of the function can be found on my [GitHub](https://github.com/bbcon/kfoldcv4ts/blob/master/R/accuracy.kfold.R)) or by running `kfoldcv4ts::accuracy.kfold` in the console once the package has been installed. In a few words, the function splits the data in k training and test folds. It then estimates a VAR on each fold and returns various out-of-sample measures for each fold. The last element of the list is the k-th fold and uses the whole dataset (minus the `n_ahead` last observations that are used as a test dataset) for the training of the model.
```{r, echo = TRUE, warning = FALSE}
kfoldcv4ts::accuracy.kfold(df.VAR = data_VAR, k=3, n_ahead = 6, lags = opt.lags.AIC, var_index = 1)
```

We now compare the following three models in terms of MSE (using `k=3`). `model1` is the baseline method with the optimal number of lags chosen according to the AIC criterion while `model2` uses the optimal number of lags according to the HQ criterion. `model3` does not the 6th variable `r colnames(USMacroSW[5])` of the dataset to estimate the VAR.
```{r}
model1 = kfoldcv4ts::accuracy.kfold(df.VAR = data_VAR, k=3, n_ahead = 6, lags = opt.lags.AIC, var_index = 1)
model2 = kfoldcv4ts::accuracy.kfold(df.VAR = data_VAR, k=3, n_ahead = 6, lags = opt.lags.HQ, var_index = 1)
model3 = kfoldcv4ts::accuracy.kfold(df.VAR = data_VAR[,1:5], k=3, n_ahead = 6, lags = 6, var_index = 1)
```

We summarise the results in the following table:
```{r}
f.extract = function(x){x[2,2]}
m1 = rbind(lapply(model1, f.extract))
m2 = rbind(lapply(model2, f.extract))
m3 = rbind(lapply(model3, f.extract))

table_summary = data.frame(rbind(m1,m2,m3), row.names = c('Model 1', 'Model 2', 'Model 3'))
colnames(table_summary) = c('k=1','k=2','k=3') 
```
```{r}
kableExtra::kable(table_summary, caption = 'MSE for each model and each fold') %>% 
  kableExtra::kable_styling()
```

We can note several interesting things. First, Model 1 seems to be the best performing in terms of MSE when estimated on the largest possible dataset (i.e. $k=3$). Additionally, we can note that more data significantly improves the fit of the model (from 1.96 to 0.14). We can also see that model 2 seems to perform particularly badly when the training dataset is relatively small (k=1). In terms of average MSE over the 3 folds, Model 3 seems to perform the best and may be considered as a good alternative to Model 1, even though it has one regressor less. This provides an example on the risk of overfitting.

# References
